# ðŸš€ Inference Framework / KVcache

- [**dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching**](https://arxiv.org/abs/2506.06295) (2025.5, SJTU)
- [**Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding**](https://arxiv.org/abs/2505.22618) (2025.7, Nvidia)
- [**DPad: Efficient Diffusion Language Models with Suffix Dropout**](https://arxiv.org/abs/2508.14148) (2025.8, Duke University)
- [**dParallel: Learnable Parallel Decoding for dLLMs**](https://arxiv.org/abs/2509.26488) (2025.9, NUS)
- [**d2Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching**](https://arxiv.org/abs/2509.23094) (2025.9, Southeast University)
- [**Power Up Diffusion LLMs: Dayâ€‘0 Support for LLaDA 2.0**](https://lmsys.org/blog/2025-12-19-diffusion-llm/) (2025.12, Ant Group DeepXPU Team, SGLang Team)
