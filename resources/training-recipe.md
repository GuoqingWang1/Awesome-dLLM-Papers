# ðŸ“‹ Training Recipe (Pre-Training & SFT)

## Training from Scratch
- [**DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models**](https://arxiv.org/abs/2211.15029) (ACL 2022, FDU)
- [**SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control**](https://arxiv.org/abs/2210.17432) (ACL 2023, UW)
- [**Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution**](https://arxiv.org/abs/2310.16834) (ICLR 2024, Stanford)
- [**Simple and Effective Masked Diffusion Language Models**](https://arxiv.org/abs/2406.07524) (NIPS 2024, Cornell Tech)
- [**Large Language Diffusion Models**](https://arxiv.org/abs/2502.09992) (NIPS 2025, RUC Gaoling)
- [**Mercury: Ultra-Fast Language Models Based on Diffusion**](https://arxiv.org/abs/2506.17298) (2025.6, Inception Labs)
- [**Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference**](https://arxiv.org/abs/2508.02193) (2025.8, Byte Dance Seed)
- [**LLaDA-MoE: A Sparse MoE Diffusion Language Model**](https://arxiv.org/abs/2509.24389) (2025.9, RUC Gaoling)
- [**Diffusion Language Models are Super Data Learners**](https://arxiv.org/abs/2511.03276) (2025.11, MDGA)

## Initialized from AR Models
- [**Scaling Diffusion Language Models via Adaptation from Autoregressive Models**](https://arxiv.org/abs/2410.17891) (ICLR 2025, HKU)
- [**Dream 7B: Diffusion Large Language Models**](https://arxiv.org/abs/2508.15487) (2025.8, HKU)
- [**Dream-Coder 7B: An Open Diffusion Language Model for Code**](https://arxiv.org/abs/2509.01142) (2025.9, HKU)
- [**Open-dLLM: Open Diffusion Large Language Model**](https://oval-shell-31c.notion.site/Open-dLLM-Open-Diffusion-Large-Language-Model-25e03bf6136480b7a4ebe3d53be9f68a) (2025.9, Duke University)
- [**OpenMoE 2: Sparse Diffusion Language Models**](https://jinjieni.notion.site/OpenMoE-2-Sparse-Diffusion-Language-Models-277d8f03a8668065a4ecd23f23bd6aac) (2025.9, MDGA)
- [**Training Diffusion Language Models at Scale using Autoregressive Models**](https://arxiv.org/abs/2508.02193) (2025.10, Radical Numerics Inc)
- [**SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation**](https://arxiv.org/abs/2510.06303) (2025.10, Shanghai AI Lab)
- [**LLaDA2.0: Scaling Up Diffusion Language Models to 100B**](https://arxiv.org/abs/2512.15745) (2025.12, Ant Group)
- [**LLaDA2.1: Speeding Up Text Diffusion via Token Editing**](https://arxiv.org/abs/2602.08676) (2026.2, Ant Group)