# ðŸŽ¯ Decoding

- [**Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models**](https://arxiv.org/abs/2402.07754) (NIPS 2024, HKU)
- [**Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models**](https://arxiv.org/abs/2505.10446) (NIPS 2025, ZJU)
- [**Path Planning for Masked Diffusion Model Sampling**](https://arxiv.org/abs/2502.03540) (2025.3, Duke University)
- [**Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models**](https://arxiv.org/abs/2503.09573) (ICLR 2025, Cornell Tech)
- [**Encoder-Decoder Diffusion Language Models for Efficient Training and Inference**](https://arxiv.org/abs/2510.22852) (NIPS 2025, Cornell Tech)
- [**KLASS: KL-Guided Fast Inference in Masked Diffusion Models**](https://arxiv.org/abs/2511.05664) (NIPS 2025, KAIST AI)
- [**Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking**](https://arxiv.org/abs/2505.24857) (NIPS 2025, FAIR)
- [**Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs**](https://arxiv.org/abs/2507.18578) (2025.7, SJTU)
- [**Beyond Fixed: Training-Free Variable-Length Denoising for Diffusion Large Language Models**](https://arxiv.org/abs/2508.00819) (2025.8, CUHK)
- [**Empirical Analysis of Decoding Biases in Masked Diffusion Models**](https://arxiv.org/abs/2508.13021) (2025.8, Northeastern University, China)
- [**Sequential Diffusion Language Models**](https://arxiv.org/abs/2509.24007) (2025.9, Shanghai AI Lab)
- [**Self Speculative Decoding for Diffusion Large Language Models**](https://arxiv.org/abs/2510.04147) (2025.10, SJTU)
- [**Beyond Confidence: Adaptive and Coherent Decoding for Diffusion Language Models**](https://arxiv.org/abs/2512.02044) (2025.11, CUHK)
- [**TiDAR: Think in Diffusion, Talk in Autoregression**](https://arxiv.org/abs/2511.08923) (2025.11, Nvidia)
- [**Learning Unmasking Policies for Diffusion Language Models**](https://arxiv.org/abs/2512.09106) (2025.12, Apple)
- [**ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding**](https://arxiv.org/abs/2512.13586) (2025.12, RUC Gaoling)
- [**Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed**](https://arxiv.org/abs/2512.14067) (2025.12, Nvidia)
- [**DEER: Draft with Diffusion, Verify with Autoregressive Models**](https://arxiv.org/abs/2512.15176) (2025.12, THU)
- [**Sequential Diffusion Language Models**](https://arxiv.org/abs/2509.24007) (2025.12, WeChat)
- [**WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference**](https://arxiv.org/abs/2512.22737) (2025.12, Shanghai AI Lab)
- [**CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models**](https://arxiv.org/abs/2601.02236) (2026.1, Princeton)
- [**Streaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding**](https://arxiv.org/abs/2601.17917) (2026.1, Beijing Institute of Technology)
- [**Focus-dLLM: Accelerating Long-Context Diffusion LLM Inference via Confidence-Guided Context Focusing**](https://arxiv.org/abs/2602.02159) (2026.2, Beihang University)